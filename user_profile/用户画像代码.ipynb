{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###将重采样样本进行合并\n",
    "import pandas as pd\n",
    "import glob\n",
    "path = glob.glob('user_tag_T60.csv')\n",
    "\n",
    "# whole_data = glob.glob(path+'*.csv')\n",
    "train_resam = pd.DataFrame()\n",
    "for filename in path:\n",
    "    fil = pd.read_csv(filename, encoding='gb18030')\n",
    "    train_resam = train_resam.append(fil, ignore_index=True)\n",
    "train_resam = train_resam.rename(columns={'Unnamed: 0': 'ID'})\n",
    "train_resam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resam = train_resam[['ID', 'age', 'Gender', 'Education', 'Query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_resam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resam_part = train_resam.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_resam_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resam_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###数据处理\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "np.random.seed(1)\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn import svm\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import RidgeClassifier, Lasso\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "# 返回平均长度\n",
    "def mean_len(L):\n",
    "    num = len(L) - 1 if len(L) > 1 else 1\n",
    "    sum_len = sum(L) - max(L)\n",
    "    return round(float(sum_len) / num, 1)\n",
    "\n",
    "\n",
    "train = pd.read_table('user_tag_query.10W.TRAIN',\n",
    "                      encoding='gb18030',\n",
    "                      header=None,\n",
    "                      sep='\\n',\n",
    "                      names=['col'])\n",
    "test = pd.read_table('user_tag_query.10W.TEST',\n",
    "                     encoding='gb18030',\n",
    "                     header=None,\n",
    "                     sep='\\n',\n",
    "                     names=['col'])\n",
    "train['ID'] = train['col'].apply(lambda x: x.split('\\t')[0])\n",
    "train['age'] = train['col'].apply(lambda x: x.split('\\t')[1])\n",
    "train['Gender'] = train['col'].apply(lambda x: x.split('\\t')[2])\n",
    "train['Education'] = train['col'].apply(lambda x: x.split('\\t')[3])\n",
    "train['Query'] = train['col'].apply(lambda x: ','.join(x.split('\\t')[4::]))\n",
    "# 把重采样的数据和真实数据进行合并\n",
    "train = pd.concat([train, train_resam_part])\n",
    "# 将数据随机打乱\n",
    "train = shuffle(train)\n",
    "# 筛选除部分数据（用于演示）\n",
    "train = train.sample(300)\n",
    "test = test.sample(100)\n",
    "\n",
    "# 统计特征\n",
    "train['mean'] = train['Query'].apply(\n",
    "    lambda x: mean_len([len(i) for i in x.split(',')]))\n",
    "train['max'] = train['Query'].apply(\n",
    "    lambda x: max([len(i) for i in x.split(',')]))\n",
    "train['std'] = train['Query'].apply(\n",
    "    lambda x: round(np.array(([len(i) for i in x.split(',')])).std(), 1))\n",
    "train['sum'] = train['Query'].apply(lambda x: np.array(\n",
    "    ([len(i) for i in x.split(',')])).sum())\n",
    "\n",
    "# 转换数据类型\n",
    "train['age'] = train['age'].astype(int)\n",
    "train['Gender'] = train['Gender'].astype(int)\n",
    "train['Education'] = train['Education'].astype(int)\n",
    "\n",
    "test['ID'] = test['col'].apply(lambda x: x.split('\\t')[0])\n",
    "test['Query'] = test['col'].apply(lambda x: ','.join(x.split('\\t')[1::]))\n",
    "\n",
    "# 统计特征\n",
    "\n",
    "test['mean'] = test['Query'].apply(\n",
    "    lambda x: mean_len([len(i) for i in x.split(',')]))\n",
    "test['max'] = test['Query'].apply(\n",
    "    lambda x: max([len(i) for i in x.split(',')]))\n",
    "test['std'] = test['Query'].apply(\n",
    "    lambda x: round(np.array(([len(i) for i in x.split(',')])).std(), 1))\n",
    "test['sum'] = test['Query'].apply(lambda x: np.array(\n",
    "    ([len(i) for i in x.split(',')])).sum())\n",
    "\n",
    "train_data = train.drop('col', axis=1)\n",
    "test_data = test.drop('col', axis=1)\n",
    "test_data.insert(1, 'age', 0)\n",
    "test_data.insert(2, 'Gender', 0)\n",
    "test_data.insert(3, 'Education', 0)\n",
    "test_data['TT'] = 'test'\n",
    "train_data['TT'] = 'train'\n",
    "\n",
    "train = train_data.drop(list(\n",
    "    train_data[(train_data['age'] == 0) | (train_data['Gender'] == 0) |\n",
    "               (train_data['Education'] == 0)].index),\n",
    "                        axis=0)\n",
    "# 根据预测结果及样本分布，删除age为6，edu=1,2的所有数据，\n",
    "# 选择性处理：删除后并无改善\n",
    "data = pd.concat([train, test_data])\n",
    "data = data.reset_index().drop('index', axis=1)\n",
    "data = data.drop(data[data['age'] == 6].index, axis=0)\n",
    "data = data.drop(data[data['Education'] == 1].index, axis=0)\n",
    "data = data.drop(data[data['Education'] == 2].index, axis=0)\n",
    "#data = data.drop(data[(data['age']==1)&(data['Education']==2)].index,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入停用词典\n",
    "jieba.analyse.set_stop_words('stopwords.txt')\n",
    "\n",
    "# 分词 生成词表\n",
    "def fenci(data):\n",
    "    list_fenci = []\n",
    "    for content in data.index:\n",
    "        feature_seg = jieba.cut(data['Query'][content])\n",
    "        seg_join = ','.join(feature_seg)\n",
    "        list_fenci.append(seg_join)\n",
    "    return list_fenci\n",
    "\n",
    "# 启用多线程\n",
    "pool0 = ThreadPool(8)\n",
    "l_data = [data]\n",
    "fenci = pool0.map(fenci, l_data)\n",
    "data['jieba_fenci'] = fenci[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化\n",
    "vectorizer1 = CountVectorizer(\n",
    "    # 词组切分的长度范围\n",
    "    ngram_range=(1, 2),\n",
    "    # 控制范围\n",
    "    min_df=18,\n",
    "    max_df=0.75,\n",
    ")\n",
    "vectorizer2 = CountVectorizer(\n",
    "    ngram_range=(1, 2), \n",
    "    min_df=25, \n",
    "    max_df=0.6)\n",
    "vectorizer3 = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=18,\n",
    "    max_df=0.85,\n",
    ")  \n",
    "X_vec1 = vectorizer1.fit_transform(data['jieba_fenci'].values)\n",
    "X_vec2 = vectorizer2.fit_transform(data['jieba_fenci'].values)\n",
    "X_vec3 = vectorizer3.fit_transform(data['jieba_fenci'].values)\n",
    "\n",
    "# 得到不同参数下的值\n",
    "X1 = TfidfTransformer(use_idf=False, smooth_idf=True,\n",
    "                      sublinear_tf=0.6).fit_transform(X_vec1)\n",
    "X2 = TfidfTransformer(use_idf=True, smooth_idf=True,\n",
    "                      sublinear_tf=0.8).fit_transform(X_vec2)\n",
    "X3 = TfidfTransformer(use_idf=True, smooth_idf=True,\n",
    "                      sublinear_tf=0.6).fit_transform(X_vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行标准化\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "mean1 = minmax.fit_transform(data['mean'].values.reshape(-1, 1))\n",
    "max1 = minmax.fit_transform(data['max'].values.reshape(-1, 1))\n",
    "std1 = minmax.fit_transform(data['std'].values.reshape(-1, 1))\n",
    "sum1 = minmax.fit_transform(data['sum'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = preprocessing.MinMaxScaler()\n",
    "mean1 = minmax.fit_transform(data['mean'].values.reshape(-1, 1))\n",
    "max1 = minmax.fit_transform(data['max'].values.reshape(-1, 1))\n",
    "std1 = minmax.fit_transform(data['std'].values.reshape(-1, 1))\n",
    "sum1 = minmax.fit_transform(data['sum'].values.reshape(-1, 1))\n",
    "\n",
    "cs = csr_matrix(\n",
    "    np.hstack((mean1, std1,\n",
    "               max1, sum1)))\n",
    "X1 = hstack((X1, cs), format='csr')\n",
    "X2 = hstack((X2, cs), format='csr')\n",
    "X3 = hstack((X3, cs), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_a = np.asarray(data['age'].values)\n",
    "y_g = np.asarray(data['Gender'].values)\n",
    "y_e = np.asarray(data['Education'].values)\n",
    "# 特征选择（使用chi2可以保持稀疏性质）\n",
    "# K表示保留多少特征\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "X_new1 = SelectKBest(chi2, k=X1.shape[1] // 7).fit_transform(X1, y_a)\n",
    "X_new2 = SelectKBest(chi2, k=X2.shape[1] // 6).fit_transform(X2, y_g)\n",
    "X_new3 = SelectKBest(chi2, k=X3.shape[1] // 7).fit_transform(X3, y_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_new2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# array化\n",
    "y1 = np.asarray(data[data['TT'] == 'train']['age'].values)\n",
    "y2 = np.asarray(data[data['TT'] == 'train']['Gender'].values)\n",
    "y3 = np.asarray(data[data['TT'] == 'train']['Education'].values)\n",
    "# 划分训练和测试\n",
    "data_train1, data_test1, target_train1, target_test1 = model_selection.train_test_split(\n",
    "    X_new1[0:(data.shape[0]-100)], y1)\n",
    "data_train2, data_test2, target_train2, target_test2 = model_selection.train_test_split(\n",
    "    X_new2[0:(data.shape[0]-100)], y2)\n",
    "data_train3, data_test3, target_train3, target_test3 = model_selection.train_test_split(\n",
    "    X_new3[0:(data.shape[0]-100)], y3)\n",
    "\n",
    "# 使用随机梯度下降\n",
    "sgd1 = SGDClassifier(penalty='elasticnet', n_jobs=-1, random_state=1)\n",
    "sgd1.fit(data_train1, target_train1)\n",
    "predict1 = sgd1.predict(data_test1)\n",
    "\n",
    "# 使用线性回归\n",
    "lr2 = LogisticRegression(n_jobs=-1, random_state=1)\n",
    "lr2.fit(data_train2, target_train2)\n",
    "predict2 = lr2.predict(data_test2)\n",
    "\n",
    "# 使用随机梯度下降\n",
    "sgd3 = SGDClassifier(penalty='elasticnet', n_jobs=-1, random_state=1)\n",
    "sgd3.fit(data_train3, target_train3)\n",
    "predict3 = sgd3.predict(data_test3)\n",
    "\n",
    "precision_score1 = precision_score(target_test1, predict1, average='micro')\n",
    "precision_score2 = precision_score(target_test2, predict2, average='micro')\n",
    "precision_score3 = precision_score(target_test3, predict3, average='micro')\n",
    "print(\"\\nModel Report\")\n",
    "print(\"precision_score : %.4g\" %\n",
    "      ((precision_score1 + precision_score2 + precision_score3) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###svm可以使结果达到0.703+\n",
    "import time\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "def predict(X):\n",
    "    svm = SVC(kernel=\"linear\")\n",
    "    svm.fit(X[1][0:(data.shape[0] - 100)], X[0])\n",
    "    prediction = svm.predict(X[1][(data.shape[0] - 100):data.shape[0]])\n",
    "    return prediction\n",
    "\n",
    "\n",
    "x1 = [y1, X_new1]\n",
    "x2 = [y2, X_new2]\n",
    "x3 = [y3, X_new3]\n",
    "L1 = [x1, x2, x3]\n",
    "pool = ThreadPool(8)\n",
    "result = pool.map(predict, L1)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(\"Time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape[0] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['ID'] = data[data['TT'] == 'test']['ID']\n",
    "submission['age'] = result[0]\n",
    "submission['Gender'] = result[1]\n",
    "submission['Education'] = result[2]\n",
    "submission.to_csv('12.10svm.csv',\n",
    "                  sep=' ',\n",
    "                  index=False,\n",
    "                  header=None,\n",
    "                  encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###stacking 非常耗时\n",
    "import random\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def run(data):\n",
    "    X = data[0]\n",
    "    Y = data[1]\n",
    "\n",
    "    dev_cutoff = 260\n",
    "    X_dev = X[:dev_cutoff]\n",
    "    Y_dev = Y[:dev_cutoff]\n",
    "    X_test = X[dev_cutoff:]\n",
    "    Y_test = Y[dev_cutoff:]\n",
    "\n",
    "    n_folds = 5\n",
    "\n",
    "    clfs = [\n",
    "        SVC(kernel=\"linear\"),\n",
    "        svm.LinearSVC(),\n",
    "        SGDClassifier(loss='hinge'),\n",
    "        SGDClassifier(penalty='elasticnet'),\n",
    "        MultinomialNB(),\n",
    "        BernoulliNB(),\n",
    "        RidgeClassifier(),\n",
    "    ]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=16)\n",
    "    skf = skf.split(X_dev, Y_dev)\n",
    "\n",
    "    blend_train = np.zeros((X_dev.shape[0], len(clfs)))\n",
    "    blend_test = np.zeros((X_test.shape[0], len(clfs)))\n",
    "\n",
    "    print('X_test.shape = %s' % (str(X_test.shape)))\n",
    "    print('blend_train.shape = %s' % (str(blend_train.shape)))\n",
    "    print('blend_test.shape = %s' % (str(blend_test.shape)))\n",
    "\n",
    "    # print(list(skf))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print('Training classifier [%s]' % (j))\n",
    "        blend_test_j = np.zeros((X_test.shape[0], 5))\n",
    "        for i, (train_index, cv_index) in enumerate(skf):\n",
    "            print('Fold [%s]' % (i))\n",
    "\n",
    "            X_train = X_dev[train_index]\n",
    "            Y_train = Y_dev[train_index]\n",
    "            X_cv = X_dev[cv_index]\n",
    "            Y_cv = Y_dev[cv_index]\n",
    "\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            blend_train[cv_index, j] = clf.predict(X_cv)\n",
    "            blend_test_j[:, i] = clf.predict(X_test)\n",
    "\n",
    "        blend_test[:, j] = blend_test_j.mean(1)\n",
    "\n",
    "    print('Y_dev.shape = %s' % (Y_dev.shape))\n",
    "\n",
    "    bclf = LogisticRegression()\n",
    "    bclf.fit(blend_train, Y_dev)\n",
    "\n",
    "    Y_test_predict = bclf.predict(blend_test)\n",
    "\n",
    "    return Y_test_predict\n",
    "\n",
    "\n",
    "l1 = [X_new1, y1]\n",
    "l2 = [X_new2, y2]\n",
    "l3 = [X_new3, y3]\n",
    "list1 = [l1,l2,l3]\n",
    "pool = ThreadPool(8)\n",
    "stacking_result = pool.map(run, list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['ID'] = test['ID']\n",
    "submission1['age'] = stacking_result[0]\n",
    "submission1['Gender'] = stacking_result[1]\n",
    "submission1['Education'] = stacking_result[2]\n",
    "submission1.to_csv('12.02stacking.csv',\n",
    "                   sep=' ',\n",
    "                   index=False,\n",
    "                   header=None,\n",
    "                   encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
